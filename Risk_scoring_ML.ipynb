{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anucodes-hub/hybrid-aml-detection-engine/blob/main/Risk_scoring_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      #                                                   THIS IS THE MAIN FINAL MODIFIED CODE\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "import shap\n",
        "\n",
        "# =========================================================\n",
        "# 1. Synthetic dataset generator (with stronger signal)\n",
        "# =========================================================\n",
        "\n",
        "def generate_customer_risk_dataset(n: int = 5000, seed: int = 42) -> pd.DataFrame:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # --- Core KYC / customer features ---\n",
        "    country_risk = np.random.choice(\n",
        "        [5, 10, 20, 40, 60, 80],\n",
        "        size=n,\n",
        "        p=[0.2, 0.25, 0.25, 0.15, 0.1, 0.05],\n",
        "    )\n",
        "\n",
        "    is_pep = np.random.binomial(1, 0.03, size=n)\n",
        "    occupation_risk = np.random.choice(\n",
        "        [5, 10, 20, 40, 60, 80],\n",
        "        size=n,\n",
        "        p=[0.25, 0.25, 0.2, 0.15, 0.1, 0.05],\n",
        "    )\n",
        "\n",
        "    face_match_score = np.clip(\n",
        "        np.random.normal(90, 5, size=n),\n",
        "        60,\n",
        "        100,\n",
        "    ).astype(int)\n",
        "\n",
        "    kyc_quality = np.clip(\n",
        "        np.random.normal(85, 7, size=n),\n",
        "        50,\n",
        "        100,\n",
        "    ).astype(int)\n",
        "\n",
        "    failed_attempts = np.random.poisson(0.3, size=n)\n",
        "\n",
        "    avg_txn_amount_30d = np.random.lognormal(\n",
        "        mean=10,\n",
        "        sigma=0.6,\n",
        "        size=n,\n",
        "    ).astype(int)\n",
        "\n",
        "    txn_count_7d = np.random.poisson(10, size=n)\n",
        "\n",
        "    # --- Receiver / counterparty features ---\n",
        "    receiver_type = np.random.choice(\n",
        "        [0, 1, 2, 3],\n",
        "        size=n,\n",
        "        p=[0.5, 0.2, 0.2, 0.1],\n",
        "    )\n",
        "\n",
        "    receiver_risk = np.random.choice(\n",
        "        [10, 30, 50, 70, 90],\n",
        "        size=n,\n",
        "        p=[0.3, 0.25, 0.2, 0.15, 0.1],\n",
        "    )\n",
        "\n",
        "    is_receiver_new = np.random.binomial(1, 0.4, size=n)\n",
        "\n",
        "    receiver_country_risk = np.random.choice(\n",
        "        [5, 20, 40, 60, 80],\n",
        "        size=n,\n",
        "        p=[0.3, 0.25, 0.2, 0.15, 0.1],\n",
        "    )\n",
        "\n",
        "    # --- Geo / channel features ---\n",
        "    is_cross_border = np.random.binomial(1, 0.25, size=n)\n",
        "\n",
        "    channel_risk = np.random.choice(\n",
        "        [10, 30, 50, 70, 90],\n",
        "        size=n,\n",
        "        p=[0.4, 0.25, 0.2, 0.1, 0.05],\n",
        "    )\n",
        "\n",
        "    geo_distance_score = np.random.choice(\n",
        "        [10, 30, 50, 70, 90],\n",
        "        size=n,\n",
        "        p=[0.4, 0.25, 0.2, 0.1, 0.05],\n",
        "    )\n",
        "\n",
        "    # --- Behavioural / relationship features ---\n",
        "    days_since_first_txn = np.random.randint(1, 365, size=n)\n",
        "    txn_to_high_risk_geo_30d = np.random.poisson(0.5, size=n)\n",
        "    receiver_txn_count_30d = np.random.poisson(3, size=n)\n",
        "    alerts_last_90d = np.random.poisson(0.4, size=n)\n",
        "\n",
        "    # Flags for regimes\n",
        "    very_high_amount = (avg_txn_amount_30d > 200000).astype(float)\n",
        "    new_short_history = ((days_since_first_txn < 30) & (is_receiver_new == 1)).astype(float)\n",
        "    high_risk_corridor = ((country_risk >= 60) & (receiver_country_risk >= 60)).astype(float)\n",
        "\n",
        "    # --- Latent risk to build labels (richer, with interactions) ---\n",
        "    latent_risk = (\n",
        "        0.18 * (country_risk / 100)\n",
        "        + 0.18 * (receiver_country_risk / 100)\n",
        "        + 0.20 * is_pep\n",
        "        + 0.16 * (occupation_risk / 100)\n",
        "        + 0.20 * (receiver_risk / 100)\n",
        "        + 0.14 * is_receiver_new\n",
        "        + 0.16 * is_cross_border\n",
        "        + 0.16 * (channel_risk / 100)\n",
        "        + 0.16 * (txn_to_high_risk_geo_30d / 5)\n",
        "        + 0.18 * (alerts_last_90d / 5)\n",
        "        - 0.15 * (face_match_score / 100)\n",
        "        - 0.15 * (kyc_quality / 100)\n",
        "        + 0.20 * very_high_amount\n",
        "        + 0.18 * new_short_history\n",
        "        + 0.20 * high_risk_corridor\n",
        "    )\n",
        "\n",
        "    # scale to [0, 1]\n",
        "    latent_risk = (latent_risk - latent_risk.min()) / (\n",
        "        latent_risk.max() - latent_risk.min()\n",
        "    )\n",
        "\n",
        "    # deterministic label with margin band\n",
        "    is_high_risk = np.zeros(n, dtype=int)\n",
        "    is_high_risk[latent_risk >= 0.65] = 1\n",
        "    is_high_risk[latent_risk <= 0.35] = 0\n",
        "\n",
        "    mid_band = (latent_risk > 0.35) & (latent_risk < 0.65)\n",
        "    noisy_labels = np.random.binomial(1, latent_risk[mid_band])\n",
        "    is_high_risk[mid_band] = noisy_labels\n",
        "\n",
        "    # IDs\n",
        "    person_id = [f\"user_{i:05d}\" for i in range(n)]\n",
        "    transaction_id = [f\"txn_{i:07d}\" for i in range(n)]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"person_id\": person_id,\n",
        "        \"transaction_id\": transaction_id,\n",
        "        \"country_risk\": country_risk,\n",
        "        \"is_pep\": is_pep,\n",
        "        \"occupation_risk\": occupation_risk,\n",
        "        \"face_match_score\": face_match_score,\n",
        "        \"kyc_quality\": kyc_quality,\n",
        "        \"failed_attempts\": failed_attempts,\n",
        "        \"avg_txn_amount_30d\": avg_txn_amount_30d,\n",
        "        \"txn_count_7d\": txn_count_7d,\n",
        "        \"receiver_type\": receiver_type,\n",
        "        \"receiver_risk\": receiver_risk,\n",
        "        \"is_receiver_new\": is_receiver_new,\n",
        "        \"receiver_country_risk\": receiver_country_risk,\n",
        "        \"is_cross_border\": is_cross_border,\n",
        "        \"channel_risk\": channel_risk,\n",
        "        \"geo_distance_score\": geo_distance_score,\n",
        "        \"days_since_first_txn\": days_since_first_txn,\n",
        "        \"txn_to_high_risk_geo_30d\": txn_to_high_risk_geo_30d,\n",
        "        \"receiver_txn_count_30d\": receiver_txn_count_30d,\n",
        "        \"alerts_last_90d\": alerts_last_90d,\n",
        "        \"is_high_risk\": is_high_risk,\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# 2. Train Random Forest risk model (tuned)\n",
        "# =========================================================\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    \"country_risk\",\n",
        "    \"is_pep\",\n",
        "    \"occupation_risk\",\n",
        "    \"face_match_score\",\n",
        "    \"kyc_quality\",\n",
        "    \"failed_attempts\",\n",
        "    \"avg_txn_amount_30d\",\n",
        "    \"txn_count_7d\",\n",
        "    \"receiver_type\",\n",
        "    \"receiver_risk\",\n",
        "    \"is_receiver_new\",\n",
        "    \"receiver_country_risk\",\n",
        "    \"is_cross_border\",\n",
        "    \"channel_risk\",\n",
        "    \"geo_distance_score\",\n",
        "    \"days_since_first_txn\",\n",
        "    \"txn_to_high_risk_geo_30d\",\n",
        "    \"receiver_txn_count_30d\",\n",
        "    \"alerts_last_90d\",\n",
        "]\n",
        "\n",
        "TARGET_COL = \"is_high_risk\"\n",
        "\n",
        "df = generate_customer_risk_dataset()\n",
        "\n",
        "X = df[FEATURE_COLS]\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf_base = RandomForestClassifier(\n",
        "    n_estimators=600,\n",
        "    class_weight=\"balanced_subsample\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "param_dist = {\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 3, 5],\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    rf_base,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=12,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "rf = search.best_estimator_\n",
        "print(\"Best RF params:\", search.best_params_)\n",
        "\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"Random Forest ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# =========================================================\n",
        "# 3. Isolation Forest anomaly model\n",
        "# =========================================================\n",
        "\n",
        "IF_FEATURE_COLS = [\n",
        "    \"avg_txn_amount_30d\",\n",
        "    \"txn_count_7d\",\n",
        "    \"receiver_type\",\n",
        "    \"receiver_risk\",\n",
        "    \"is_receiver_new\",\n",
        "    \"receiver_country_risk\",\n",
        "    \"is_cross_border\",\n",
        "    \"channel_risk\",\n",
        "    \"geo_distance_score\",\n",
        "    \"days_since_first_txn\",\n",
        "    \"txn_to_high_risk_geo_30d\",\n",
        "    \"receiver_txn_count_30d\",\n",
        "    \"alerts_last_90d\",\n",
        "]\n",
        "\n",
        "X_if = df[IF_FEATURE_COLS].values\n",
        "\n",
        "if_model = IsolationForest(\n",
        "    n_estimators=500,\n",
        "    max_samples=\"auto\",\n",
        "    contamination=0.05,\n",
        "    max_features=1.0,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "if_model.fit(X_if)\n",
        "\n",
        "scores_raw_all = if_model.score_samples(X_if)\n",
        "scores_flipped_all = -scores_raw_all\n",
        "IF_MIN = scores_flipped_all.min()\n",
        "IF_MAX = scores_flipped_all.max()\n",
        "\n",
        "def compute_anomaly_score(if_model, params: dict) -> dict:\n",
        "    x_if = pd.DataFrame([params])[IF_FEATURE_COLS].values\n",
        "    raw = if_model.score_samples(x_if)[0]\n",
        "    flipped = -raw\n",
        "    norm = (flipped - IF_MIN) / (IF_MAX - IF_MIN)\n",
        "    score = int(round(norm * 100))\n",
        "\n",
        "    if score < 30:\n",
        "        level = \"NORMAL\"\n",
        "    elif score < 70:\n",
        "        level = \"ELEVATED\"\n",
        "    else:\n",
        "        level = \"HIGH\"\n",
        "\n",
        "    return {\n",
        "        \"anomaly_score\": score,\n",
        "        \"anomaly_level\": level,\n",
        "    }\n",
        "\n",
        "# =========================================================\n",
        "# 3b. Simple AML rule engine\n",
        "# =========================================================\n",
        "\n",
        "def rule_engine_score(params: dict) -> dict:\n",
        "    score = 0\n",
        "\n",
        "    # --- Existing High-risk country check ---\n",
        "    if params[\"country_risk\"] >= 60 or params[\"receiver_country_risk\"] >= 60:\n",
        "        score += 20\n",
        "    if params[\"country_risk\"] >= 60 and params[\"receiver_country_risk\"] >= 60:\n",
        "        score += 15\n",
        "\n",
        "    # --- LOOPHOLE 3 FIX: TEMPORAL VELOCITY CHECK ---\n",
        "    # 1. High Frequency Check: Many transactions in a short window\n",
        "    if params.get(\"txn_count_7d\", 0) > 40:\n",
        "        score += 25  # Significant penalty for \"Smurfing\" frequency\n",
        "\n",
        "    # 2. New Account Burst Check: Rapid activity on a fresh account\n",
        "    if params.get(\"days_since_first_txn\", 365) < 7 and params.get(\"txn_count_7d\", 0) > 10:\n",
        "        score += 30  # Classic \"Mule Account\" footprint\n",
        "\n",
        "    # --- Existing Behaviour & KYC checks ---\n",
        "    if params[\"alerts_last_90d\"] >= 3:\n",
        "        score += 20\n",
        "    if params[\"avg_txn_amount_30d\"] > 200000:\n",
        "        score += 15\n",
        "    if params[\"kyc_quality\"] < 70 or params[\"face_match_score\"] < 80:\n",
        "        score += 10\n",
        "\n",
        "    score = max(0, min(100, score))\n",
        "    level = \"HIGH\" if score >= 70 else \"MEDIUM\" if score >= 30 else \"LOW\"\n",
        "\n",
        "    return {\"rule_score\": score, \"rule_level\": level}\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4. SHAP explainability (better background, robust)\n",
        "# =========================================================\n",
        "\n",
        "bg_low = X_train[y_train == 0].sample(min(250, (y_train == 0).sum()), random_state=42)\n",
        "bg_high = X_train[y_train == 1].sample(min(250, (y_train == 1).sum()), random_state=42)\n",
        "rf_background = pd.concat([bg_low, bg_high], axis=0)\n",
        "\n",
        "rf_explainer = shap.TreeExplainer(\n",
        "    rf,\n",
        "    rf_background,\n",
        "    feature_perturbation=\"interventional\",\n",
        "    model_output=\"probability\",\n",
        ")\n",
        "\n",
        "def explain_rf(params: dict) -> dict:\n",
        "    x = pd.DataFrame([params])[FEATURE_COLS]\n",
        "    shap_values_output = rf_explainer.shap_values(x)\n",
        "    exp = rf_explainer.expected_value\n",
        "\n",
        "    if isinstance(shap_values_output, list):\n",
        "        phi = np.array(shap_values_output[1][0])\n",
        "        if isinstance(exp, (list, np.ndarray)):\n",
        "            exp_val = exp[1]\n",
        "        else:\n",
        "            exp_val = exp\n",
        "    else:\n",
        "        phi = np.array(shap_values_output[0])\n",
        "        if isinstance(exp, (list, np.ndarray)):\n",
        "            exp_val = exp[0]\n",
        "        else:\n",
        "            exp_val = exp\n",
        "\n",
        "    return {\n",
        "        \"expected_value\": float(np.array(exp_val).ravel()[0]),\n",
        "        \"shap_values\": dict(zip(FEATURE_COLS, phi.ravel().tolist())),\n",
        "    }\n",
        "\n",
        "def explain_rf_advanced(params: dict) -> dict:\n",
        "    x = pd.DataFrame([params])[FEATURE_COLS]\n",
        "    shap_values_output = rf_explainer.shap_values(x)\n",
        "    shap_interactions_output = rf_explainer.shap_interaction_values(x)\n",
        "\n",
        "    if isinstance(shap_values_output, list):\n",
        "        phi = np.array(shap_values_output[1][0])\n",
        "        inter = np.array(shap_interactions_output[1][0])\n",
        "    else:\n",
        "        phi = np.array(shap_values_output[0])\n",
        "        inter = np.array(shap_interactions_output[0])\n",
        "\n",
        "    main_effects = dict(zip(FEATURE_COLS, phi.ravel().tolist()))\n",
        "\n",
        "    interaction_pairs = []\n",
        "    for i, fi in enumerate(FEATURE_COLS):\n",
        "        for j in range(i + 1, len(FEATURE_COLS)):\n",
        "            fj = FEATURE_COLS[j]\n",
        "            interaction_pairs.append(((fi, fj), abs(inter[i, j])))\n",
        "    interaction_pairs.sort(key=lambda t: t[1], reverse=True)\n",
        "    top_interactions = [\n",
        "        {\"pair\": pair, \"strength\": float(strength)}\n",
        "        for (pair, strength) in interaction_pairs[:3]\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"main_effects\": main_effects,\n",
        "        \"top_interactions\": top_interactions,\n",
        "    }\n",
        "\n",
        "# SHAP for Isolation Forest (optional)\n",
        "if_explainer = shap.TreeExplainer(if_model, X_if)\n",
        "\n",
        "def explain_if(params: dict) -> dict:\n",
        "    x = pd.DataFrame([params])[IF_FEATURE_COLS]\n",
        "    shap_vals = if_explainer.shap_values(x)\n",
        "    exp = if_explainer.expected_value\n",
        "\n",
        "    shap_vals_row = np.array(shap_vals)[0]\n",
        "    exp_val = float(np.array(exp).ravel()[0])\n",
        "\n",
        "    return {\n",
        "        \"expected_value\": exp_val,\n",
        "        \"shap_values\": dict(zip(IF_FEATURE_COLS, shap_vals_row.tolist())),\n",
        "    }\n",
        "\n",
        "# =========================================================\n",
        "# 5. Helpers: risk score, status, and SHAP-based reasons\n",
        "# =========================================================\n",
        "\n",
        "def compute_risk_score(model, params: dict) -> dict:\n",
        "    x = pd.DataFrame([params])[FEATURE_COLS]\n",
        "    proba_high = model.predict_proba(x)[0, 1]\n",
        "    score = int(round(proba_high * 100))\n",
        "\n",
        "    if score < 30:\n",
        "        level = \"LOW\"\n",
        "    elif score < 70:\n",
        "        level = \"MEDIUM\"\n",
        "    else:\n",
        "        level = \"HIGH\"\n",
        "\n",
        "    return {\n",
        "        \"probability_high_risk\": float(proba_high),\n",
        "        \"risk_score\": score,\n",
        "        \"risk_level\": level,\n",
        "    }\n",
        "\n",
        "def top_reasons_from_shap(shap_values: dict, top_n: int = 2) -> list:\n",
        "    sorted_feats = sorted(\n",
        "        shap_values.items(),\n",
        "        key=lambda kv: abs(kv[1]),\n",
        "        reverse=True,\n",
        "    )\n",
        "    return sorted_feats[:top_n]\n",
        "\n",
        "REASON_LABELS = {\n",
        "    \"country_risk\": \"country risk is high\",\n",
        "    \"is_pep\": \"customer is a politically exposed person\",\n",
        "    \"occupation_risk\": \"occupation has higher risk\",\n",
        "    \"receiver_risk\": \"receiver risk is high\",\n",
        "    \"is_receiver_new\": \"receiver is new for this customer\",\n",
        "    \"receiver_country_risk\": \"receiver’s country risk is high\",\n",
        "    \"is_cross_border\": \"transaction is cross-border\",\n",
        "    \"channel_risk\": \"channel risk is high\",\n",
        "    \"alerts_last_90d\": \"customer has multiple past alerts\",\n",
        "    \"txn_to_high_risk_geo_30d\": \"many transactions to high-risk countries\",\n",
        "}\n",
        "\n",
        "def build_reason_text(shap_values: dict, params: dict) -> str:\n",
        "    ranked = sorted(\n",
        "        shap_values.items(),\n",
        "        key=lambda kv: abs(kv[1]),\n",
        "        reverse=True,\n",
        "    )\n",
        "\n",
        "    filtered = []\n",
        "    for feat, val in ranked:\n",
        "        if feat == \"is_pep\" and params.get(\"is_pep\", 0) == 0:\n",
        "            continue\n",
        "        if feat == \"is_cross_border\" and params.get(\"is_cross_border\", 0) == 0:\n",
        "            continue\n",
        "        if feat == \"is_receiver_new\" and params.get(\"is_receiver_new\", 0) == 0:\n",
        "            continue\n",
        "        filtered.append((feat, val))\n",
        "\n",
        "    if not filtered:\n",
        "        return \"Model detected elevated risk based on multiple factors.\"\n",
        "\n",
        "    (f1, v1) = filtered[0]\n",
        "    (f2, v2) = filtered[1] if len(filtered) > 1 else (None, 0.0)\n",
        "\n",
        "    label1 = REASON_LABELS.get(f1, f1)\n",
        "    label2 = REASON_LABELS.get(f2, f2) if f2 else None\n",
        "\n",
        "    strength = abs(v1)\n",
        "    if strength < 0.02:\n",
        "        prefix = \"Risk is slightly influenced by \"\n",
        "    elif strength < 0.05:\n",
        "        prefix = \"Risk is moderately influenced by \"\n",
        "    else:\n",
        "        prefix = \"Risk is strongly driven by \"\n",
        "\n",
        "    if label2 and abs(v1) - abs(v2) < 0.01:\n",
        "        return f\"{prefix}{label1} and {label2}.\"\n",
        "    else:\n",
        "        return f\"{prefix}{label1}.\"\n",
        "\n",
        "# Official FIU-IND Grounds of Suspicion (GoS) Mapping\n",
        "GOS_MAPPING = {\n",
        "    \"country_risk\": \"GOS_101\",          # High Risk Jurisdictions\n",
        "    \"is_pep\": \"GOS_402\",                # PEP Involvement\n",
        "    \"avg_txn_amount_30d\": \"GOS_201\",    # Unusual Volume\n",
        "    \"txn_count_7d\": \"GOS_204\",          # High Velocity/Structuring\n",
        "    \"kyc_quality\": \"GOS_501\",           # Suspicious Documents\n",
        "    \"face_match_score\": \"GOS_502\"       # Identity Mismatch\n",
        "}\n",
        "\n",
        "def derive_regulatory_status(risk_score, anomaly_score, rule_score, params):\n",
        "    \"\"\"\n",
        "    Maps AI scores to official FIU-IND reporting priorities and GoS tags.\n",
        "    \"\"\"\n",
        "    aggregate = max(risk_score, anomaly_score, rule_score)\n",
        "\n",
        "    # 1. Determine Report Priority\n",
        "    if aggregate >= 90:\n",
        "        status, priority = \"STR_P1\", \"URGENT_TERROR_FINANCING\"\n",
        "    elif aggregate >= 75:\n",
        "        status, priority = \"STR_P2\", \"HIGH_ML_SUSPICION\"\n",
        "    elif aggregate >= 50:\n",
        "        status, priority = \"MANUAL_REVIEW\", \"MEDIUM\"\n",
        "    else:\n",
        "        status, priority = \"ACCEPTED\", \"LOW\"\n",
        "\n",
        "    # 2. Extract GoS Tags based on triggering features\n",
        "    triggered_gos = []\n",
        "    if params.get(\"country_risk\", 0) >= 60: triggered_gos.append(GOS_MAPPING[\"country_risk\"])\n",
        "    if params.get(\"is_pep\"): triggered_gos.append(GOS_MAPPING[\"is_pep\"])\n",
        "    if params.get(\"txn_count_7d\", 0) > 40: triggered_gos.append(GOS_MAPPING[\"txn_count_7d\"])\n",
        "    if params.get(\"avg_txn_amount_30d\", 0) > 200000: triggered_gos.append(GOS_MAPPING[\"avg_txn_amount_30d\"])\n",
        "\n",
        "    return {\n",
        "        \"regulatory_status\": status,\n",
        "        \"priority_level\": priority,\n",
        "        \"gos_tags\": triggered_gos\n",
        "    }\n",
        "\n",
        "\n",
        "'''def derive_status(risk_score: int, anomaly_score: int, rule_score: int) -> str:\n",
        "    \"\"\"\n",
        "    Combine RF risk, anomaly, and rules into status.\n",
        "    \"\"\"\n",
        "    aggregate = max(risk_score, anomaly_score, rule_score)\n",
        "\n",
        "    if aggregate >= 90:\n",
        "        return \"REPORTED\"\n",
        "    if aggregate >= 80:\n",
        "        return \"FLAGGED\"\n",
        "    if aggregate >= 65:\n",
        "        return \"MANUAL_REVIEW\"\n",
        "    if aggregate >= 45:\n",
        "        return \"UNDER_OBSERVATION\"\n",
        "    if 30 <= aggregate < 45:\n",
        "        return \"ACCEPTED_UNDER_OBSERVATION\"\n",
        "    return \"ACCEPTED\" '''\n",
        "\n",
        "# =========================================================\n",
        "# 6. Compact API\n",
        "# =========================================================\n",
        "\n",
        "def score_compact(params: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Returns fully formatted summary:\n",
        "      - risk_score (0–100)\n",
        "      - anomaly_score (0–100)\n",
        "      - rule_score (0–100)\n",
        "      - risk_level (LOW / MEDIUM / HIGH)\n",
        "      - anomaly_level (NORMAL / ELEVATED / HIGH)\n",
        "      - rule_level (LOW / MEDIUM / HIGH)\n",
        "      - explanation (string)\n",
        "      - status (ACCEPTED, UNDER_OBSERVATION, MANUAL_REVIEW, FLAGGED, REPORTED, etc.)\n",
        "    \"\"\"\n",
        "    risk = compute_risk_score(rf, params)\n",
        "    anomaly = compute_anomaly_score(if_model, params)\n",
        "    rules = rule_engine_score(params)\n",
        "\n",
        "    rf_shap = explain_rf(params)\n",
        "    explanation = build_reason_text(rf_shap[\"shap_values\"], params)\n",
        "\n",
        "    # FIX: Use the new regulatory status instead of the old derive_status\n",
        "    reg_status = derive_regulatory_status(\n",
        "        risk[\"risk_score\"],\n",
        "        anomaly[\"anomaly_score\"],\n",
        "        rules[\"rule_score\"],\n",
        "        params\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"risk_score\": risk[\"risk_score\"],\n",
        "        \"anomaly_score\": anomaly[\"anomaly_score\"],\n",
        "        \"rule_score\": rules[\"rule_score\"],\n",
        "        \"risk_level\": risk[\"risk_level\"],\n",
        "        \"anomaly_level\": anomaly[\"anomaly_level\"],\n",
        "        \"rule_level\": rules[\"rule_level\"],\n",
        "        \"explanation\": explanation,\n",
        "        \"status\": reg_status[\"regulatory_status\"], # Use official regulatory status\n",
        "        \"priority\": reg_status[\"priority_level\"]\n",
        "    }\n",
        "\n",
        "# =========================================================\n",
        "# 7. Example usage\n",
        "# =========================================================\n",
        "\n",
        "example_input = {\n",
        " \"country_risk\": 80,              # High-risk country (e.g., Sanctioned)\n",
        "    \"is_pep\": 1,                     # Customer is a Politically Exposed Person\n",
        "    \"occupation_risk\": 80,           # High-risk industry (e.g., Arms or Crypto)\n",
        "    \"face_match_score\": 65,          # Low match (potential identity fraud)\n",
        "    \"kyc_quality\": 55,               # Poor documentation\n",
        "    \"failed_attempts\": 3,            # Multiple failed KYC tries\n",
        "    \"avg_txn_amount_30d\": 250000,    # Massive volume spike (> 200,000)\n",
        "    \"txn_count_7d\": 45,              # High velocity/frequency\n",
        "    \"receiver_type\": 3,              # High-risk receiver type\n",
        "    \"receiver_risk\": 90,             # Receiver is already blacklisted\n",
        "    \"is_receiver_new\": 1,            # First time sending to this person\n",
        "    \"receiver_country_risk\": 80,     # Sending to a high-risk corridor\n",
        "    \"is_cross_border\": 1,            # International movement\n",
        "    \"channel_risk\": 90,              # High-risk channel (e.g., Dark Web wallet)\n",
        "    \"geo_distance_score\": 90,        # Transaction from an impossible distance\n",
        "    \"days_since_first_txn\": 5,       # Very new account (high risk for \"mules\")\n",
        "    \"txn_to_high_risk_geo_30d\": 8,   # Pattern of sending to risky areas\n",
        "    \"receiver_txn_count_30d\": 20,    # Receiver is receiving too many txns\n",
        "    \"alerts_last_90d\": 5,            # Repeat offender/Multiple past alerts\n",
        "\n",
        "}\n",
        "\n",
        "summary = score_compact(example_input)\n",
        "print(summary)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ALERTS SENT TO GOVT SERVER THROUGH WEBHOOK\n",
        "#We aren't just sending data; we are sending Authenticated Intelligence.\n",
        "#By using HMAC-SHA256 signatures, we ensure our AML alerts meet the highest cybersecurity standards for financial infrastructure,\n",
        "#preventing any adversarial tampering between our AI and the Regulator\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "import hmac\n",
        "import hashlib\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# 1. SECURITY SETTINGS (In production, these come from environment variables)\n",
        "WEBHOOK_URL = \"https://webhook.site/b687ed92-0daf-4454-842e-64734c026e04\"\n",
        "SECRET_KEY = \"HACKATHON_GOVT_SECRET_2026\"  # The shared secret key\n",
        "\n",
        "summary = score_compact(example_input)\n",
        "def send_secure_report(summary_data, original_input):\n",
        "    \"\"\"\n",
        "    Wraps the AI summary in metadata and signs it with a digital signature.\n",
        "    \"\"\"\n",
        "    reg_data = derive_regulatory_status(\n",
        "        summary_data[\"risk_score\"],\n",
        "        summary_data[\"anomaly_score\"],\n",
        "        summary_data[\"rule_score\"],\n",
        "        original_input\n",
        "    )\n",
        "    # Create Payload\n",
        "    payload = {\n",
        "        \"metadata\": {\n",
        "            \"report_id\": str(uuid.uuid4()),\n",
        "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "            \"entity\": \"AstuComply\",\n",
        "            \"fiu_report_type\": \"STR\"  # Suspicious Transaction Report\n",
        "        },\n",
        "        \"regulatory_analysis\": reg_data, # NEW: Includes official GoS Tags and Priority\n",
        "        \"ai_analysis\": summary_data\n",
        "    }\n",
        "\n",
        "    # Convert payload to string for signing\n",
        "    payload_str = json.dumps(payload)\n",
        "\n",
        "    # --- LOOPHOLE 4 FIX: GENERATE DIGITAL SIGNATURE ---\n",
        "    # We sign the data using the secret key so the Govt knows it's us\n",
        "    signature = hmac.new(\n",
        "        SECRET_KEY.encode(),\n",
        "        payload_str.encode(),\n",
        "        hashlib.sha256\n",
        "    ).hexdigest()\n",
        "\n",
        "    # Add Security Headers\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-AML-Signature\": signature,  # The digital \"seal\"\n",
        "        \"X-API-KEY\": \"SECRET_KEY_12345\" # Second layer of authentication\n",
        "    }\n",
        "\n",
        "    # Send the Secure POST\n",
        "    response = requests.post(WEBHOOK_URL, data=payload_str, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"✅ SUCCESS: Encrypted & Signed Report sent!\")\n",
        "        print(f\"Digital Signature (HMAC): {signature[:10]}...\")\n",
        "    else:\n",
        "        print(f\"❌ SECURITY ERROR: Code {response.status_code}\")\n",
        "\n",
        "# Execute\n",
        "send_secure_report(summary, example_input)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2RR5yLvJfXp",
        "outputId": "ee248f53-95e9-485f-e286-9f7619bd043b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RF params: {'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Random Forest ROC-AUC: 0.8752531301034295\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.91       835\n",
            "           1       0.56      0.45      0.50       165\n",
            "\n",
            "    accuracy                           0.85      1000\n",
            "   macro avg       0.73      0.69      0.71      1000\n",
            "weighted avg       0.84      0.85      0.84      1000\n",
            "\n",
            "{'risk_score': 65, 'anomaly_score': 150, 'rule_score': 100, 'risk_level': 'MEDIUM', 'anomaly_level': 'HIGH', 'rule_level': 'HIGH', 'explanation': 'Risk is strongly driven by country risk is high and customer is a politically exposed person.', 'status': 'STR_P1', 'priority': 'URGENT_TERROR_FINANCING'}\n",
            "✅ SUCCESS: Encrypted & Signed Report sent!\n",
            "Digital Signature (HMAC): d8cb2b75e7...\n"
          ]
        }
      ]
    }
  ]
}